# Unstructured Data Pipeline on AWS

![Python](https://img.shields.io/badge/Python-3.9%2B-blue)
![AWS](https://img.shields.io/badge/AWS-Serverless-orange)
![License](https://img.shields.io/badge/License-MIT-green)

This project sets up a fully automated, serverless pipeline on AWS to ingest, store, and catalog unstructured data (CSV, JSON, XML, TXT, etc.) using AWS Lambda, Glue, and S3. This enables downstream analytics using services like Amazon QuickSight.

## âœ… Supported File Types

- CSV
- JSON
- XML
- TXT

---

## ğŸ¯ Purpose

To automate the ingestion of various unstructured data formats into a structured format suitable for querying, analysis, and dashboarding.

---

## âš™ï¸ Architecture Overview

1. **S3** â€“ Input files uploaded to a source bucket.
2. **Lambda** â€“ Triggered by upload, validates and stages data.
3. **Glue Crawler** â€“ Classifies and catalogs data to Glue Data Catalog.
4. **Glue Job** â€“ Transforms data into Parquet and loads to target S3.
5. **Amazon Athena** â€“ Enables SQL querying of cataloged data.
6. **Amazon QuickSight** â€“ Connects to Athena for visualization and dashboarding.

---

## ğŸ“ Project Structure

```
unstructured-data-pipeline/
â”œâ”€â”€ data_pipeline.py             # Orchestrates the full pipeline
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ __init__.py              # Module initializer
â”‚   â”œâ”€â”€ s3_setup.py              # S3 buckets setup
â”‚   â”œâ”€â”€ lambda_deploy.py         # Lambda function deployment
â”‚   â””â”€â”€ glue_setup.py            # Glue crawler and job creation
â””â”€â”€ lambda_function/
    â””â”€â”€ lambda_handler.py        # Lambda function code
```

---

## ğŸš€ Getting Started

```bash
# Install dependencies
pip install -r requirements.txt

# Run the pipeline
python data_pipeline.py
```

---

## ğŸ§ª Example Usage

1. Upload `sample.csv` to `raw-data-bucket`.
2. Lambda stages it to `staging-bucket`.
3. Glue Crawler catalogs it.
4. Glue Job transforms it to Parquet format and stores it in `processed-data-bucket`.
5. Data is queryable in Athena.
6. Connect QuickSight to Athena for dashboards.

---

## ğŸ”§ Requirements

- Python 3.9+
- AWS credentials configured (`~/.aws/credentials`)
- IAM role with permissions for S3, Lambda, Glue, Athena, and QuickSight

---

## ğŸ“Š Final Output

- Cleaned, transformed Parquet files in `processed-data-bucket`
- Catalog tables accessible via AWS Athena
- Visual dashboards in Amazon QuickSight

---

## ğŸ“Œ Notes

- Add support for more formats in `lambda_handler.py` if needed.
- Extend for streaming sources using Amazon Kinesis.
- Set up a QuickSight dataset to directly connect to the Athena database generated by Glue.
